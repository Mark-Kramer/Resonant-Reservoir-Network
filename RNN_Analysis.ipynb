{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b47ebda-6332-4259-aeb6-a48d849bd2ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 0. Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029813-e398-490b-aeb9-f32f96dadfa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load the data & prepare it\n",
    "# --------------------------------------------------------- ---------\n",
    "readdata = sio.loadmat('./dat/simulated_neural_data_for_Optuna.mat')\n",
    "data     = readdata['data']\n",
    "labels   = readdata['label'].ravel()\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Import ReservoirNetwork\n",
    "# ------------------------------------------------------------------\n",
    "import importlib\n",
    "import ReservoirNetwork\n",
    "importlib.reload(ReservoirNetwork)\n",
    "from ReservoirNetwork import ReservoirNetwork, extract_reservoir_features\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Fix seed so reservoir deterministic.\n",
    "#-------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Define the objective function for Optuna\n",
    "# ------------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    # 1) Sample hyperparameters\n",
    "    Fs                   = 1000\n",
    "    fstep                = trial.suggest_int(  'fstep', 1, 10)\n",
    "    sigma                = trial.suggest_float('sigma', 1e-4, 1e-1, log=True)\n",
    "    sparsity             = trial.suggest_float('sparsity', 0.0, 1.0)\n",
    "    spectral_radius      = trial.suggest_float('spectral_radius', 0.1, 1.5)\n",
    "    base_geometric_ratio = trial.suggest_float('base_geometric_ratio', 0.5, 0.99)\n",
    "\n",
    "    # 2) Build reservoir & extract _all_ features\n",
    "    res_net      = ReservoirNetwork(Fs=Fs, fstep=fstep, sigma=sigma,\n",
    "                                    sparsity=sparsity, spectral_radius=spectral_radius,\n",
    "                                    base_geometric_ratio=base_geometric_ratio,\n",
    "                                    random_state=42) # make the reservoir itself reproducible\n",
    "    X_feat       = extract_reservoir_features(res_net, data)\n",
    "    X_scaled     = StandardScaler().fit_transform(X_feat)\n",
    "\n",
    "    # 3) Cross-validated logistic regression\n",
    "    clf          = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10_000)\n",
    "    skf          = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores       = cross_val_score(clf, X_scaled, labels, cv=skf, scoring='accuracy', n_jobs=1)  # ← only one job for CV\n",
    "\n",
    "    mean_accuracy = scores.mean()\n",
    "    trial.report(mean_accuracy, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return mean_accuracy\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Run optimization \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "from datetime import datetime\n",
    "date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "save_name  = f\"sqlite:///simulated_optuna_results_{date_today}.db\"\n",
    "\n",
    "# Create and optimize the studyz\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "                            study_name=\"sim_optuna\", storage=save_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b686c9-d802-45f3-85c3-a0d82bf1e445",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# 1. Simulated neural rhythms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d59e6-6f99-4724-bf4c-ee3da2e90a69",
   "metadata": {},
   "source": [
    "## Run: reservoir or simple power values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d224303-d9eb-48e5-9746-3256ce79ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import accuracy_score, confusion_matrix\n",
    "from ReservoirNetwork        import ReservoirNetwork, extract_reservoir_features\n",
    "from getFFTfeatures          import extract_fft_power_features\n",
    "\n",
    "feature_choice = 'reservoir' # OR --> feature_choise = 'fft_power'\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load Data\n",
    "# ------------------------------------------------------------------\n",
    "readdata = sio.loadmat('dat/simulated_neural_data.mat')\n",
    "data     = readdata['data']\n",
    "labels   = readdata['label'].ravel()\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "K = 100\n",
    "accuracy         = np.zeros(K)\n",
    "correct_counts   = np.zeros([K,4])\n",
    "confuse_matrix   = []\n",
    "\n",
    "for k in np.arange(K):\n",
    "    split_seed = 100 + k\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Create the ReservoirNetwork using default optimized parameters\n",
    "    # ------------------------------------------------------------------\n",
    "    Fs  = 1000\n",
    "    res_net = ReservoirNetwork(Fs=Fs, random_state=split_seed)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Split the data into training and testing sets\n",
    "    # ------------------------------------------------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.2, stratify=labels, random_state=split_seed\n",
    "    )\n",
    "    print(\"Training data label distribution:\")\n",
    "    unique_labels_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    for ul, ct in zip(unique_labels_train, counts_train):\n",
    "        print(f\"Label {ul}: {ct} samples\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Extract Features for Training\n",
    "    # ------------------------------------------------------------------\n",
    "    if feature_choice   == 'reservoir':\n",
    "        X_train_features                  = extract_reservoir_features(res_net, X_train)\n",
    "    elif feature_choice == 'fft_power':\n",
    "        X_train_features                  = extract_fft_power_features(X_train, fs=Fs, fmin=res_net.frange[0], fmax=res_net.frange[-1])\n",
    "        X_train_features                  = np.vstack(X_train_features)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature_choice! Must be 'reservoir' or 'fft_power'.\")\n",
    "    print(f\"Training feature extraction complete ({feature_choice}).\")\n",
    "    print(f\"Training feature shape: {X_train_features.shape}\")\n",
    "    scaler                  = StandardScaler()\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Train the Classifier\n",
    "    # ------------------------------------------------------------------\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "    clf.fit(X_train_features_scaled, y_train);\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Extract Features for Test Set\n",
    "    # ------------------------------------------------------------------\n",
    "    if feature_choice   == 'reservoir':\n",
    "        X_test_features                   = extract_reservoir_features(res_net, X_test)\n",
    "    elif feature_choice == 'fft_power':\n",
    "        X_test_features                   = extract_fft_power_features(X_test,  fs=Fs, fmin=res_net.frange[0], fmax=res_net.frange[-1])\n",
    "        X_test_features                   = np.vstack(X_test_features)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature_choice! Must be 'reservoir' or 'fft_power'.\")\n",
    "\n",
    "    X_test_features_scaled = scaler.transform(X_test_features)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 7) Predict and Evaluate\n",
    "    # ------------------------------------------------------------------\n",
    "    y_pred      = clf.predict(X_test_features_scaled)\n",
    "    accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "    print(k,accuracy[k])\n",
    "\n",
    "    cm          = confusion_matrix(y_test, y_pred)\n",
    "    confuse_matrix.append(cm)\n",
    "\n",
    "    print(\"Correct classifications per test label:\")\n",
    "    unique_labels_test = np.unique(y_test)\n",
    "    correct_count      = np.zeros(4)\n",
    "    for index,ul in enumerate(unique_labels_test):\n",
    "        test_mask = (y_test == ul)\n",
    "        correct_count[index] = np.sum((y_pred == ul) & (y_test == ul))\n",
    "        print(f\"Label {ul}: Correctly identified {correct_count[index]} times out of {np.sum(test_mask)}\")\n",
    "\n",
    "    correct_counts[k,:] = correct_count\n",
    "\n",
    "# Convert the list of confusion matrices into a NumPy array\n",
    "confuse_matrix = np.array(confuse_matrix)\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "from datetime import datetime\n",
    "date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "out_fname  = f\"SIM_accuracy_results_{feature_choice}_{date_today}.pkl\"\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'accuracy': accuracy, \n",
    "    'correct_counts': correct_counts, \n",
    "    'confuse_matrix': confuse_matrix, \n",
    "    'scaler': scaler, \n",
    "    'res_net': res_net, \n",
    "    'X_train': X_train, \n",
    "    'y_train': y_train, \n",
    "    'X_test': X_test, \n",
    "    'y_test': y_test,\n",
    "    'X_train_features': X_train_features, \n",
    "    'X_test_features': X_test_features, \n",
    "    'y_pred': y_pred\n",
    "}\n",
    "\n",
    "with open(out_fname, \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfd08d-8e52-4cd8-89d3-462a8ffe7590",
   "metadata": {},
   "source": [
    "## Run: Increase frequency step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915a731-5f07-4feb-9efa-63869e46965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import pickle\n",
    "from ReservoirNetwork        import ReservoirNetwork, extract_reservoir_features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import accuracy_score, confusion_matrix\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load Data\n",
    "# ------------------------------------------------------------------\n",
    "readdata = sio.loadmat('dat/simulated_neural_data.mat')\n",
    "data     = readdata['data']\n",
    "labels   = readdata['label'].ravel()\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "K      = 100\n",
    "fsteps = np.arange(2,21)\n",
    "\n",
    "for fstep in fsteps:\n",
    "\n",
    "    accuracy         = np.zeros(K)\n",
    "    correct_counts   = np.zeros([K,4])\n",
    "    confuse_matrix   = []\n",
    "    \n",
    "    for k in np.arange(K):\n",
    "        split_seed = 100 + k\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Create the ReservoirNetwork using default optimized parameters\n",
    "        # ------------------------------------------------------------------\n",
    "        res_net = ReservoirNetwork(Fs=1000, fstep=fstep)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Split the data into training and testing sets\n",
    "        # ------------------------------------------------------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data, labels, test_size=0.2, stratify=labels, random_state=split_seed\n",
    "        )\n",
    "        print(\"Training data label distribution:\")\n",
    "        unique_labels_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "        for ul, ct in zip(unique_labels_train, counts_train):\n",
    "            print(f\"Label {ul}: {ct} samples\")\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 4) Extract Features for Training\n",
    "        # ------------------------------------------------------------------\n",
    "        X_train_features        = extract_reservoir_features(res_net, X_train)\n",
    "        scaler                  = StandardScaler()\n",
    "        X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 5) Train the Classifier\n",
    "        # ------------------------------------------------------------------\n",
    "        clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "        clf.fit(X_train_features_scaled, y_train);\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 6) Extract Features for Test Set\n",
    "        # ------------------------------------------------------------------\n",
    "        X_test_features        = extract_reservoir_features(res_net, X_test)\n",
    "        X_test_features_scaled = scaler.transform(X_test_features)\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 7) Predict and Evaluate\n",
    "        # ------------------------------------------------------------------\n",
    "        y_pred      = clf.predict(X_test_features_scaled)\n",
    "        accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "        print(k,accuracy[k])\n",
    "    \n",
    "        cm          = confusion_matrix(y_test, y_pred)\n",
    "        confuse_matrix.append(cm)\n",
    "    \n",
    "        print(\"Correct classifications per test label:\")\n",
    "        unique_labels_test = np.unique(y_test)\n",
    "        correct_count      = np.zeros(4)\n",
    "        for index,ul in enumerate(unique_labels_test):\n",
    "            test_mask = (y_test == ul)\n",
    "            correct_count[index] = np.sum((y_pred == ul) & (y_test == ul))\n",
    "            print(f\"Label {ul}: Correctly identified {correct_count[index]} times out of {np.sum(test_mask)}\")\n",
    "    \n",
    "        correct_counts[k,:] = correct_count\n",
    "    \n",
    "    # Convert the list of confusion matrices into a NumPy array\n",
    "    confuse_matrix = np.array(confuse_matrix)\n",
    "    \n",
    "    # Get today's date in YYYY-MM-DD format\n",
    "    from datetime import datetime\n",
    "    date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "    save_name  = f\"SIM_accuracy_results_fstep_{fstep}_date_{date_today}\"\n",
    "    \n",
    "    # Save the results\n",
    "    results = {\n",
    "        'accuracy': accuracy, \n",
    "        'correct_counts': correct_counts, \n",
    "        'confuse_matrix': confuse_matrix, \n",
    "        'scaler': scaler, \n",
    "        'res_net': res_net, \n",
    "        'X_train': X_train, \n",
    "        'y_train': y_train, \n",
    "        'X_test': X_test, \n",
    "        'y_test': y_test,\n",
    "        'X_train_features': X_train_features, \n",
    "        'X_test_features': X_test_features, \n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    with open(save_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0493c8-2a55-4b64-a01a-89a75d4fb5a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. In vivo data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83707aa6-9d74-450c-9bc5-24560de3b47c",
   "metadata": {},
   "source": [
    "## Run: RRN on in vivo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d100f6a-1eb3-4dd4-a339-435ba3089594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmin= 5.0 , fmax= 508.0 , fstep= 1.0 , N nodes= 504\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import importlib\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.metrics       import accuracy_score\n",
    "\n",
    "import ReservoirNetwork\n",
    "importlib.reload(ReservoirNetwork)\n",
    "from ReservoirNetwork import ReservoirNetwork, extract_reservoir_features\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def balance_classes(X, y, seed):\n",
    "    \"\"\"\n",
    "    Given training data X,y with classes {0:'y',1:'N',2:'b'},\n",
    "    sample so that #N + #b = #y, then collapse b->N.\n",
    "    Returns (X_bal, y_bal).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # indices for each class\n",
    "    idx_y = np.where(y == 0)[0]\n",
    "    idx_N = np.where(y == 1)[0]\n",
    "    idx_b = np.where(y == 2)[0]\n",
    "    n_y   = len(idx_y)\n",
    "    if len(idx_N) < n_y//2 or len(idx_b) < n_y - n_y//2:\n",
    "        raise ValueError(\"Not enough N/b to balance y\")\n",
    "    sel_N = rng.choice(idx_N, size=n_y//2, replace=False)\n",
    "    sel_b = rng.choice(idx_b, size=n_y - n_y//2, replace=False)\n",
    "    final = np.concatenate([idx_y, sel_N, sel_b])\n",
    "    X_bal = X[final]\n",
    "    y_bal = y[final].copy()\n",
    "    # collapse b(2)->N(1)\n",
    "    y_bal[y_bal == 2] = 1\n",
    "    return X_bal, y_bal\n",
    "\n",
    "def train_and_test(res_net, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Extract reservoir features, scale, fit LR, predict and compute metrics.\n",
    "    Returns dict with accuracy, sensitivity, specificity, PPV, NPV.\n",
    "    \"\"\"\n",
    "    # 1) features\n",
    "    Xtr_feat = extract_reservoir_features(res_net, X_train)\n",
    "    Xte_feat = extract_reservoir_features(res_net, X_test)\n",
    "\n",
    "    # 2) scale & fit\n",
    "    scaler = StandardScaler().fit(Xtr_feat)\n",
    "    Xtr = scaler.transform(Xtr_feat)\n",
    "    Xte = scaler.transform(Xte_feat)\n",
    "    clf = LogisticRegression(\n",
    "        multi_class='multinomial', solver='lbfgs', max_iter=10_000\n",
    "    ).fit(Xtr, y_train)\n",
    "\n",
    "    # 3) predict\n",
    "    y_pred = clf.predict(Xte)\n",
    "    # collapse b(2)->N(1)\n",
    "    y_test[y_test == 2] = 1\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 4) confusion elements\n",
    "    pos = 0  # ‘y’\n",
    "    TP = np.sum((y_pred == pos) & (y_test == pos))\n",
    "    FN = np.sum((y_pred != pos) & (y_test == pos))\n",
    "    FP = np.sum((y_pred == pos) & (y_test != pos))\n",
    "    TN = np.sum((y_pred != pos) & (y_test != pos))\n",
    "\n",
    "    sens = TP / (TP + FN) if TP+FN else np.nan\n",
    "    spec = TN / (TN + FP) if TN+FP else np.nan\n",
    "    ppv  = TP / (TP + FP) if TP+FP else np.nan\n",
    "    npv  = TN / (TN + FN) if TN+FN else np.nan\n",
    "\n",
    "    return {\n",
    "        'accuracy':    acc,\n",
    "        'sensitivity': sens,\n",
    "        'specificity': spec,\n",
    "        'PPV':         ppv,\n",
    "        'NPV':         npv\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main script\n",
    "# ------------------------------------------------------------------\n",
    "# 1) load data\n",
    "readdata = sio.loadmat('data/EEG_classifications_3label.mat')\n",
    "data     = readdata['data'].T\n",
    "label    = readdata['label'].ravel()\n",
    "subj     = readdata['subj'].ravel()\n",
    "\n",
    "# map labels\n",
    "mapping = {'y':0,'n':1,'b':2}\n",
    "labels = np.array([mapping[l] for l in label])\n",
    "\n",
    "selected_subjects = [\n",
    "    'pBECTS003','pBECTS007','pBECTS011',\n",
    "    'pBECTS015','pBECTS033','pBECTS043'\n",
    "]\n",
    "\n",
    "K = 100\n",
    "results = {\n",
    "    'accuracy':    np.zeros((K,len(selected_subjects))),\n",
    "    'sensitivity': np.zeros((K,len(selected_subjects))),\n",
    "    'specificity': np.zeros((K,len(selected_subjects))),\n",
    "    'PPV':         np.zeros((K,len(selected_subjects))),\n",
    "    'NPV':         np.zeros((K,len(selected_subjects))),\n",
    "}\n",
    "\n",
    "# 2) outer loop over random seeds\n",
    "for k in range(K):\n",
    "    seed   = 100 + k\n",
    "    res_net = ReservoirNetwork(Fs=2035, random_state=seed)\n",
    "\n",
    "    # 3) leave-one-subject-out\n",
    "    for i, subj_id in enumerate(selected_subjects):\n",
    "        # split\n",
    "        test_idx  = np.where(subj == subj_id)[0]\n",
    "        train_idx = np.where(subj != subj_id)[0]\n",
    "        Xtr, ytr = data[train_idx], labels[train_idx]\n",
    "        Xte, yte = data[test_idx],  labels[test_idx]\n",
    "\n",
    "        # balance\n",
    "        Xtr_bal, ytr_bal = balance_classes(Xtr, ytr, seed)\n",
    "\n",
    "        # train & get metrics\n",
    "        m = train_and_test(res_net, Xtr_bal, ytr_bal, Xte, yte)\n",
    "\n",
    "        print(f\"{subj_id}: Test size {np.shape(Xte)[0]} , Accuracy {m['accuracy']:.2f} , Sensitivity {m['sensitivity']:.2f} , Specificity {m['specificity']:.2f} , PPV {m['PPV']:.2f} , NPV {m['NPV']:.2f}\")\n",
    "\n",
    "        # store\n",
    "        results['accuracy'][k,i]    = m['accuracy']\n",
    "        results['sensitivity'][k,i] = m['sensitivity']\n",
    "        results['specificity'][k,i] = m['specificity']\n",
    "        results['PPV'][k,i]         = m['PPV']\n",
    "        results['NPV'][k,i]         = m['NPV']\n",
    "\n",
    "# 4) save\n",
    "from datetime import datetime\n",
    "fname = f\"INVIVO_accuracy_results_{datetime.today():%Y-%m-%d}.pkl\"\n",
    "with open(fname,'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549d1b8-405c-4811-8c3a-a3a803652cab",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. MNIST Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff9f3d-799c-419b-8ea1-9c13d4d04721",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run: RRN on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc62b8c-6cce-4579-8434-2b83f19a87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   keras.datasets import mnist\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections           import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.metrics       import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline      import Pipeline\n",
    "from ReservoirNetwork      import ReservoirNetwork, extract_reservoir_features\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load the MNIST data\n",
    "# ------------------------------------------------------------------\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Prepare the data\n",
    "# ------------------------------------------------------------------\n",
    "# Flatten images from (60000, 28, 28) to (60000, 784)\n",
    "X_train = X_train.reshape(60000, 784).astype(np.float32)\n",
    "X_test  =  X_test.reshape(10000, 784).astype(np.float32)\n",
    "print('After flattening: ', np.shape(X_train))\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test  /= 255.0\n",
    "\n",
    "# Print sample counts\n",
    "print(\"\\nLabel distribution in training subset:\", dict(Counter(y_train)))\n",
    "print(\"Label distribution in testing subset:\", dict(Counter(y_test)))\n",
    "\n",
    "# Build pipelines for scaling & classification\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10_000))\n",
    "])\n",
    "\n",
    "K = 100\n",
    "accuracy         = np.zeros(K)\n",
    "confuse_matrix   = []\n",
    "for k in np.arange(K):\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Instantiate the ReservoirNetwork\n",
    "    # ------------------------------------------------------------------\n",
    "    res_net = ReservoirNetwork(Fs = 1000)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Extract Features for Training & Testing\n",
    "    # ------------------------------------------------------------------\n",
    "    X_train_features = extract_reservoir_features(res_net, X_train)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Train the Classifier\n",
    "    # ------------------------------------------------------------------\n",
    "    clf.fit(X_train_features, y_train)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Extract Features for Test Set\n",
    "    # ------------------------------------------------------------------\n",
    "    X_test_features         = extract_reservoir_features(res_net, X_test)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 7) Predict and Evaluate\n",
    "    # ------------------------------------------------------------------\n",
    "    y_pred = clf.predict(X_test_features)\n",
    "\n",
    "    accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "    print(k,accuracy[k])\n",
    "    \n",
    "    # Evaluate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confuse_matrix.append(cm)\n",
    "\n",
    "# Convert the list of confusion matrices into a NumPy array\n",
    "confuse_matrix = np.array(confuse_matrix)\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "from datetime import datetime\n",
    "date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "save_name  = f\"MNIST_accuracy_results_{date_today}\"\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'res_net': res_net, \n",
    "    'clf':     clf,\n",
    "    'accuracy': accuracy, \n",
    "    'confuse_matrix': confuse_matrix, \n",
    "    'X_train': X_train, \n",
    "    'y_train': y_train, \n",
    "    'X_test': X_test, \n",
    "    'y_test': y_test,\n",
    "    'X_train_features': X_train_features, \n",
    "    'X_test_features': X_test_features, \n",
    "    'y_pred': y_pred\n",
    "}\n",
    "\n",
    "with open(save_name + '.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0c94a-48dc-4f48-85bd-448a20ef7693",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run: Increase frequency step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece39cc-9ff4-4b9b-a543-1927823a6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   keras.datasets import mnist\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections           import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.metrics       import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline      import Pipeline\n",
    "from ReservoirNetwork      import ReservoirNetwork, extract_reservoir_features\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load the MNIST data\n",
    "# ------------------------------------------------------------------\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Prepare the data\n",
    "# ------------------------------------------------------------------\n",
    "# Flatten images from (60000, 28, 28) to (60000, 784)\n",
    "X_train = X_train.reshape(60000, 784).astype(np.float32)\n",
    "X_test  =  X_test.reshape(10000, 784).astype(np.float32)\n",
    "print('After flattening: ', np.shape(X_train))\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test  /= 255.0\n",
    "\n",
    "# Print sample counts\n",
    "print(\"\\nLabel distribution in training subset:\", dict(Counter(y_train)))\n",
    "print(\"Label distribution in testing subset:\", dict(Counter(y_test)))\n",
    "\n",
    "# Build pipelines for scaling & classification\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10_000))\n",
    "])\n",
    "\n",
    "fsteps = np.arange(2,11)\n",
    "K = 100\n",
    "\n",
    "for fstep in fsteps:\n",
    "\n",
    "    accuracy         = np.zeros(K)\n",
    "    confuse_matrix   = []\n",
    "    \n",
    "    for k in np.arange(K):\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Instantiate the ReservoirNetwork\n",
    "        # ------------------------------------------------------------------\n",
    "        res_net = ReservoirNetwork(Fs = 1000, fstep=fstep)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 4) Extract Features for Training & Testing\n",
    "        # ------------------------------------------------------------------\n",
    "        X_train_features = extract_reservoir_features(res_net, X_train)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 5) Train the Classifier\n",
    "        # ------------------------------------------------------------------\n",
    "        clf.fit(X_train_features, y_train)\n",
    "    \n",
    "        # ------------------------------------------------------------------\n",
    "        # 6) Extract Features for Test Set\n",
    "        # ------------------------------------------------------------------\n",
    "        X_test_features  = extract_reservoir_features(res_net, X_test)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 7) Predict and Evaluate\n",
    "        # ------------------------------------------------------------------\n",
    "        y_pred = clf.predict(X_test_features)\n",
    "    \n",
    "        accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "        print(k,accuracy[k])\n",
    "        \n",
    "        # Evaluate confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        confuse_matrix.append(cm)\n",
    "    \n",
    "    # Convert the list of confusion matrices into a NumPy array\n",
    "    confuse_matrix = np.array(confuse_matrix)\n",
    "    \n",
    "    # Get today's date in YYYY-MM-DD format\n",
    "    from datetime import datetime\n",
    "    date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "    save_name  = f\"MNIST_accuracy_results_fstep_{fstep}_date_{date_today}\"\n",
    "    \n",
    "    # Save the results\n",
    "    results = {\n",
    "        'res_net': res_net, \n",
    "        'clf':     clf,\n",
    "        'accuracy': accuracy, \n",
    "        'confuse_matrix': confuse_matrix, \n",
    "        'X_train': X_train, \n",
    "        'y_train': y_train, \n",
    "        'X_test': X_test, \n",
    "        'y_test': y_test,\n",
    "        'X_train_features': X_train_features, \n",
    "        'X_test_features': X_test_features, \n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    with open(save_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc3208-13d2-4c70-b4b3-fa1447dbfdd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Speech Commands Dataset (SCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7e466-9ae8-47ee-ada5-1387800d9c34",
   "metadata": {},
   "source": [
    "## Run: RRN on SCD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e41a95-6d14-43ab-bd7a-4e6cb3d4222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from ReservoirNetwork import ReservoirNetwork, extract_reservoir_features\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def load_speech_commands_dataset(data_path, sr=None, max_examples_per_label=None, fixed_length=None):\n",
    "    \n",
    "    labels_map = {\n",
    "        \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "        \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9\n",
    "    }\n",
    "    \n",
    "    X_audio = []\n",
    "    y = []\n",
    "    max_len = 0  # Track max length of any audio signal\n",
    "\n",
    "    # First pass: load all audio and find max length if needed\n",
    "    temp_audio = []  # Store temporary list of raw audio signals\n",
    "    for label_name, label_val in labels_map.items():\n",
    "        folder_path = os.path.join(data_path, label_name)\n",
    "        wav_files = glob.glob(os.path.join(folder_path, \"*.wav\"))\n",
    "        \n",
    "        if max_examples_per_label is not None and len(wav_files) > max_examples_per_label:\n",
    "            wav_files = np.random.choice(wav_files, size=max_examples_per_label, replace=False)\n",
    "        \n",
    "        for wav_file in wav_files:\n",
    "            audio, _ = librosa.load(wav_file, sr=sr)\n",
    "            temp_audio.append(audio)\n",
    "            y.append(label_val)\n",
    "            max_len = max(max_len, len(audio))\n",
    "    \n",
    "    # Use user-defined fixed length or the maximum length found\n",
    "    if fixed_length is None:\n",
    "        fixed_length = max_len\n",
    "\n",
    "    # Second pass: Pad or truncate and convert to NumPy array\n",
    "    for audio in temp_audio:\n",
    "        if len(audio) < fixed_length:\n",
    "            # Pad with zeros at the end\n",
    "            audio = np.pad(audio, (0, fixed_length - len(audio)), mode='constant')\n",
    "        else:\n",
    "            # Truncate to fixed length\n",
    "            audio = audio[:fixed_length]\n",
    "        X_audio.append(audio)\n",
    "\n",
    "    X_audio = np.array(X_audio)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X_audio, y\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and standardize the audio data\n",
    "# ---------------------------\n",
    "data_path  = \"data/speech_commands_v0.02\"\n",
    "Fs         = 4000  # Target sample rate\n",
    "X_audio, y = load_speech_commands_dataset(data_path, sr=Fs)\n",
    "\n",
    "# Report some diagnostics.\n",
    "print(f\"Loaded {X_audio.shape[0]} audio files with shape {X_audio.shape}.\")\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "# Zero-mean and scale to [-1, 1]\n",
    "X_audio_standardized = []\n",
    "for signal in X_audio:\n",
    "    sig0 = signal - np.mean(signal)      # Subtract the mean\n",
    "    peak = np.max(np.abs(sig0))          # Find the peak absolute value\n",
    "    if peak > 0:                         # Avoid division by zero (unlikely)\n",
    "        sig_scaled = sig0 / peak\n",
    "    else:\n",
    "        sig_scaled = sig0\n",
    "    X_audio_standardized.append(sig_scaled)\n",
    "\n",
    "# Prepare to repeat K times and save results\n",
    "K                = 100\n",
    "accuracy         = np.zeros(K)\n",
    "confuse_matrix   = []\n",
    "for k in trange(K):\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 2. Split into training and test sets\n",
    "    # ---------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_audio_standardized,\n",
    "        y, test_size=0.1, stratify=y,\n",
    "        random_state=k\n",
    "    )\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test  = np.asarray(X_test)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 3. Process data with the ReservoirNetwork\n",
    "    # ---------------------------\n",
    "    res_net         = ReservoirNetwork(Fs=Fs, random_state=k)\n",
    "    \n",
    "    # Extract reservoir features for training and test sets.\n",
    "    X_train_features = extract_reservoir_features(res_net, X_train)\n",
    "    X_test_features  = extract_reservoir_features(res_net, X_test)\n",
    "    \n",
    "    # Scale features before logistic regression.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
    "    X_test_features_scaled  = scaler.transform(X_test_features)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 4. Train a classifier on the reservoir features\n",
    "    # ---------------------------\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "    clf.fit(X_train_features_scaled, y_train);\n",
    "    \n",
    "    # ---------------------------\n",
    "    # 5. Evaluate the classifier\n",
    "    # ---------------------------\n",
    "    y_pred = clf.predict(X_test_features_scaled)\n",
    "    accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test accuracy on SDDS using Reservoir Network features:\", k,accuracy[k])\n",
    "\n",
    "    # Evaluate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confuse_matrix.append(cm)\n",
    "\n",
    "    # Timestamp results (backup in case code crashes)\n",
    "    temp_results = {\n",
    "        'res_net': res_net, \n",
    "        'accuracy': accuracy, \n",
    "        'confuse_matrix': confuse_matrix, \n",
    "    }\n",
    "    date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "    save_name  = f\"SDDS_TEMP_accuracy_results_{date_today}\"\n",
    "    with open(save_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(temp_results, f)\n",
    "\n",
    "# Convert the list of confusion matrices into a NumPy array\n",
    "confuse_matrix = np.array(confuse_matrix)\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "date_today = datetime.today().strftime('%Y-%m-%d')\n",
    "save_name  = f\"SDDS_accuracy_results_{date_today}\"\n",
    "\n",
    "# Save results after K iterations complete.\n",
    "results = {\n",
    "    'res_net': res_net, \n",
    "    'accuracy': accuracy, \n",
    "    'confuse_matrix': confuse_matrix, \n",
    "    'scaler': scaler, \n",
    "    'X_train': X_train, \n",
    "    'y_train': y_train, \n",
    "    'X_test': X_test, \n",
    "    'y_test': y_test,\n",
    "    'X_train_features': X_train_features, \n",
    "    'X_test_features': X_test_features, \n",
    "    'y_pred': y_pred\n",
    "}\n",
    "\n",
    "with open(save_name + '.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05d1d6-5696-49c8-90dc-e7e4e74a6b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
